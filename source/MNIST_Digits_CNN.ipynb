{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Digits Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import datetime\n",
    "import scipy.misc \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "try:\n",
    "    from StringIO import StringIO  # Python 2.7\n",
    "except ImportError:\n",
    "    from io import BytesIO         # Python 3.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.3.1\n"
     ]
    }
   ],
   "source": [
    "print('TensorFlow version: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code referenced from https://gist.github.com/gyglim/1f8dfb1b5c82627ae3efcfbbadb9f514\n",
    "class Logger(object):\n",
    "    \n",
    "    def __init__(self, log_dir):\n",
    "        \"\"\"Create a summary writer logging to log_dir.\"\"\"\n",
    "        self.writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "    def scalar_summary(self, tag, value, step):\n",
    "        \"\"\"Log a scalar variable.\"\"\"\n",
    "        with self.writer.as_default():\n",
    "            tf.summary.scalar(name=tag, data=value, step=step)\n",
    "        self.writer.flush()\n",
    "\n",
    "    def image_summary(self, tag, images, step):\n",
    "        \"\"\"Log a list of images.\"\"\"\n",
    "\n",
    "        for i, img in enumerate(images):\n",
    "            # Write the image to a string\n",
    "            try:\n",
    "                s = StringIO()\n",
    "            except:\n",
    "                s = BytesIO()\n",
    "            scipy.misc.toimage(img).save(s, format=\"png\")\n",
    "\n",
    "            # Create an Image object as a Summary value\n",
    "            with self.writer.as_default():\n",
    "                tf.summary.image(name='%s/%d' % (tag, i), data=s.getvalue(), step=step)\n",
    "\n",
    "        # Create and write Summary\n",
    "        self.writer.flush()\n",
    "        \n",
    "    def histo_summary(self, tag, values, step, bins=1000):\n",
    "        \"\"\"Log a histogram of the tensor of values.\"\"\"\n",
    "\n",
    "        # Create a histogram using numpy\n",
    "        counts, bin_edges = np.histogram(values, bins=bins)\n",
    "\n",
    "        # Fill the fields of the histogram proto\n",
    "        hist = tf.HistogramProto()\n",
    "        hist.min = float(np.min(values))\n",
    "        hist.max = float(np.max(values))\n",
    "        hist.num = int(np.prod(values.shape))\n",
    "        hist.sum = float(np.sum(values))\n",
    "        hist.sum_squares = float(np.sum(values**2))\n",
    "\n",
    "        # Drop the start of the first bin\n",
    "        bin_edges = bin_edges[1:]\n",
    "\n",
    "        # Add bin edges and counts\n",
    "        for edge in bin_edges:\n",
    "            hist.bucket_limit.append(edge)\n",
    "        for c in counts:\n",
    "            hist.bucket.append(c)\n",
    "\n",
    "        # Create and write Summary\n",
    "        with self.writer.as_default():\n",
    "            tf.summary.histogram(name=tag, data=hist, step=step)\n",
    "        self.writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Show Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to /Users/yuchen.zhang/.pytorch/F_MNIST_data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e7a36f5052343cf92bdfc2fa08c2024",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/yuchen.zhang/.pytorch/F_MNIST_data/FashionMNIST/raw/train-images-idx3-ubyte.gz to /Users/yuchen.zhang/.pytorch/F_MNIST_data/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to /Users/yuchen.zhang/.pytorch/F_MNIST_data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f2814fe294c4645a5232324813a4a68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/yuchen.zhang/.pytorch/F_MNIST_data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to /Users/yuchen.zhang/.pytorch/F_MNIST_data/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to /Users/yuchen.zhang/.pytorch/F_MNIST_data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5194bc86bba45cb83b0db24692f718a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting /Users/yuchen.zhang/.pytorch/F_MNIST_data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to /Users/yuchen.zhang/.pytorch/F_MNIST_data/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to /Users/yuchen.zhang/.pytorch/F_MNIST_data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da15b02b8cd6414fb9016a038d8356a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/yuchen.zhang/.pytorch/F_MNIST_data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to /Users/yuchen.zhang/.pytorch/F_MNIST_data/FashionMNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuchen.zhang/opt/anaconda3/lib/python3.7/site-packages/torchvision/datasets/mnist.py:469: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /Users/distiller/project/conda/conda-bld/pytorch_1595629430416/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "# Set normalization mean std\n",
    "mean = 0.5\n",
    "std = 0.5\n",
    "\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((mean,), (std,))\n",
    "                                ])\n",
    "\n",
    "# Set batch size\n",
    "train_batch_size = 5\n",
    "test_batch_size = 5\n",
    "\n",
    "# Download and load the training data\n",
    "trainset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=test_batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# Classes\n",
    "classes = {       0 :'T-shirt/top',\n",
    "                  1 :'Trouser',\n",
    "                  2 :'Pullover',\n",
    "                  3 :'Dress',\n",
    "                  4 :'Coat',\n",
    "                  5 :'Sandal',\n",
    "                  6 :'Shirt',\n",
    "                  7 :'Sneaker',\n",
    "                  8 :'Bag',\n",
    "                  9 :'Ankle boot'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA78AAADhCAYAAAAAjAKAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAXEQAAFxEByibzPwAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3dfcye5V038ANB1pX2bktf6Qu0pFDY3KBm2DFmQDPGMMtMpjD8YzFxRjTFlz+MziW67iUbw0Ujm5g4fFmizoxMJ67M6GTIwppRFAajgJQCpUDfS1/oGyDPX89jfPL7Xtt5e1Pu++jn8+f3vq7zPK/rOs/jPI/cyfc45dVXX20AAADQsx96vQ8AAAAAXmsmvwAAAHTP5BcAAIDumfwCAADQPZNfAAAAumfyCwAAQPdMfgEAAOieyS8AAADdM/kFAACgeya/AAAAdM/kFwAAgO6Z/AIAANA9k18AAAC6d9r/5s3r1q17daIOBAAAAL6fdevWnTKe9/nPLwAAAN0z+QUAAKB7Jr8AAAB0z+QXAACA7pn8AgAA0D2TXwAAALpn8gsAAED3TH4BAADonskvAAAA3TP5BQAAoHsmvwAAAHTvtBO5s4997GMncncn1Hve854yf/nll+N7vvGNb7xWhzMu1157bZk/++yzZX7PPfe8lofzuvvoRz866PU9n9/0xbn9gzn77LPL/Bd/8RfLfNeuXWW+fv36Mj906FCZf+ADHyjzlStXlvnnPve5Mt+8eXOZ966H8/uUU04p81dfffUEH8lov/RLvxT/ds4555T5D/1Q/X+Xxx9/vMz//M//fPiBDTCR3/Vr/bv1cG6/nlavXl3mv/3bv13mt912W5l/+9vfLvN3vvOdZf6+972vzG+++eYyb621jRs3xr/1auj5PV7+8wsAAED3TH4BAADonskvAAAA3TP5BQAAoHsmvwAAAHTvhLY9T0apzXPBggVlfuTIkTJPjcjXXXdd3PeP//iPl/nBgwfL/L/+67/KPDUnpu3MmzevzNeuXVvm6TgvuuiiMm+ttTe84Q1l/vTTT5f5jh074rYAflA/+qM/WuaXXXZZmb/lLW+J29q/f3+Zp/EtjaEf//jHy/z48eNlnu4zX/7yl8v8+uuvL/MZM2aUeWt5LP7mN79Z5t/5znfitph4qR34h3/4h8v8pZdeGrT9dA6nduDUkvv888/HffzVX/1VmS9fvrzMr7nmmkH5Qw89VOaf/OQny/zAgQNlPvS7bm34983E++AHPxj/9v73v7/Mx8bGyjydx6lJPz1H7969u8zvuuuuMr/xxhvLvLW8GsDf/d3flfkXv/jFuC3+J//5BQAAoHsmvwAAAHTP5BcAAIDumfwCAADQPZNfAAAAunfStD2/+c1vLvNly5aV+TPPPDNo+6kV9E/+5E/ie1J78yc+8Yky/4Vf+IUy37NnT5l/6lOfKvPbb7+9zH//93+/zJcuXVrmL7/8cpm3llsS3/72t5d5atq799574z6Ak9eHP/zhMk9tnkePHi3zzZs3x3288sorZb59+/YyX79+fZnPnTu3zFNjbGqlTfer9JlPOeWUMm+ttWnTppX5T//0T5d5av3/7Gc/G/fB+J12Wv14NrRl+CMf+UiZX3HFFWX+ta99rczT9TaRbr311kGv/7Vf+7Uyv+OOO8o8PfvcdNNNZT7quz799NMHv4fx+a3f+q0yv/zyy+N7nnvuuTJPK7Ok+0N6hk/Pv2n1lTlz5pT5rl27yry1PEZfe+21Zb5o0aIy/8xnPlPm6f6Q2s974j+/AAAAdM/kFwAAgO6Z/AIAANA9k18AAAC6Z/ILAABA90x+AQAA6F53Sx2l+vlzzz23zA8ePFjms2bNKvNUh378+PEf4Oj+p2PHjpX59ddfX+Yf+tCHyjxVqKfK9dmzZ5f5hRdeWObps51xxhll3lprK1euLPN9+/aVeVqK6sEHHyzz9DsAfUnLo6VlILZt21bmadmiN7zhDXHfacm2tPxbyo8cOVLmp556apmnz5aWUUl5uge0lpfnS/nChQvLfPXq1WV+//33x32fbMazpEhaSmXVqlVl/qUvfanMt27dWubvfve7476HmD59evxbesZJyzwOXWLl5ptvHpRv3LixzK+66qoy/8AHPhD3vXv37u9zdEzUUjrp2XTUb5DG3LSEWLoPpOWD0tidpDE63WNay8/eO3fuLPM3velNg44p/Q4nwxJI/vMLAABA90x+AQAA6J7JLwAAAN0z+QUAAKB7Jr8AAAB0r7u255kzZw56fWrDHBsbK/PUMpy2M6ptMzVTn3feeWWe2pXTdg4fPlzmqbEtNcul5uuzzz67zFvLv8OWLVvKfPHixYP2re0ZTg5r1qwp89SGm6Q2z9Rs2VpupU3jT3p92kd6/dCxO21n1P0ntYymY01tpen36bntOX1H6ftOTeOj/OZv/maZp5bmO++8c9B2ktQ0ns75dK6eCOnZJz3LXHLJJWV+ww03lPlNN90U93377beX+Ve/+tX4npPN0Nbg1OqcnsdHjd07duwo8xkzZpR5GkPTfWbU2DrEqPtYuubSqgLpM/zIj/xImX/ve98r89RkPfSeO5n5zy8AAADdM/kFAACgeya/AAAAdM/kFwAAgO6Z/AIAANC97tqe3/jGN5b5aafVHzU1tqVm5SRtZ1Q7WnpPOtbUVppaOw8ePBj3PeR4jhw5UubHjh0btP3WcpNkMnv27DJPTX5AX1LTZxonp0+fXuapcTc1ZI56TzKqfXTI9oe2CadxddTxp/tGek/ad2pP7Vlqqx16vmzYsCH+7R/+4R/KPLU9D5XOsam0kkJqIB/q85///OD37Ny5s8zPPffcMv+DP/iDwfuY6oaOh5dffnmZp2fQUau7DG1vTq8f2rA/1Hi2k8buAwcOlPkVV1xR5qnt+WTgP78AAAB0z+QXAACA7pn8AgAA0D2TXwAAALpn8gsAAED3umt7njVrVpkfP368zIc2v6XWuWQ8TaKpxfTQoUOD9pHanlOTddpOan9M7dOjpM+cvtclS5aU+WOPPTZ430w9t912W/zb/fffX+af+tSnynzu3LllvmfPnuEHxglz+umnl/nQ9vtt27aVeVohoLV8f0gNoKndNL3+1FNPLfN0D9i9e3eZp/tbGj9by+N9astO0j2X/3b33XeX+Z133hnfc+ONNw7ax9BzL+WT0dD286HNvUO/u9ZaW7BgQZn/+7//e5n/2Z/9WZnv378/7mOqG9p+fumll5Z5ej4ctSpLWiUgPc+mcyCN0encS2Nx2n66X7WW731pH2mOsGbNmjJPLeejVqnphf/8AgAA0D2TXwAAALpn8gsAAED3TH4BAADonskvAAAA3euu7Xn27NllfuzYsTJPbXRLly4t8x07dpT54cOHyzy1tbWW299S22ZqnUvG08ZcSW2oo5r80r5nzJhR5i+99FKZj2rzox9r164t83e9613xPYsWLSrzTZs2lXm6Frdv3z4oT22R6bpN1/mb3/zmMm+tta1bt5b529/+9jL/9re/Xeb33Xdf3MdUkdow0+954YUXlvnDDz9c5mlMam14U276rdNYObTtOX0XKU+toK21NnPmzDJ/29veVuapmXj58uVxHyebq6++usxXrVpV5jfddNOE7XsqtTcPlT7b0DbhodsfZc6cOWWexpNrrrmmzG+99dbB++7V4sWLy/zRRx8t81FN/elvQxvCU9P00Ofr9Po01reW7w/Tpk0r8zTer1ix4vsc3Q9mPK3ok5X//AIAANA9k18AAAC6Z/ILAABA90x+AQAA6J7JLwAAAN3rru05Nbm9/PLLZf7CCy+U+TnnnDNovxs2bCjzsbGxQdtpLbfCpc+WjGrCG7Lf8Xjve99b5p/97GfLPLXUaXue3Ia2/6XGyw996ENlvnnz5rjvhx56qMy//vWvx/dUUsPkiy++WOZDW51TE+83vvGNeExpXErXT2oY7aHtOX221LD/jne8o8y/9a1vlXlaCaC1PIamZtB0DgxtGE0WLlxY5mmcfPrpp+O2UqtzaoHev3//oH2fjFavXl3mu3btKvNbbrklbuvXf/3Xy/xnfuZnyvzAgQPf5+hOHqkJ/oorrijza6+9tsw/+MEPxn089thjZZ6eNd/znveUeQ9tz0PHt3PPPbfM0++WvtNRY09a+WXnzp1lnu7T6T6TPltqb075qKbkBQsWlHm6J6bn6PTMkuY56b6h7RkAAACmEJNfAAAAumfyCwAAQPdMfgEAAOieyS8AAADd667tOTW8HTx4cNB2UrvcqaeeOuj1qb1u1HteeeWVMn/ppZfKfGjDdZLa7lKr3ag26aEtpqlJdN68eWWemvOGfuapZDI27Q3d96c//ekyf+SRR8o8tRS21tpll11W5o8//niZv+td7yrz1Ky8fPnyMt+zZ0+ZpxbJ9PqLL764zFtr7TOf+UyZv/Wtby3zT3ziE3FbU0Uaf1ILfRoz5syZU+apifdrX/taPKZly5aV+dDxLZmosTu9Pp2TrbV2/vnnl3m6z6TPnPZ9Mo7Ra9euLfNt27aV+Y4dO+K23vKWt5T5Aw88UOap2T2NP2ncS7/PrFmzyjw9r7SWV7tI12hqsU2N/Ol5LD3vDW1v37JlS5m3lu996ZiuvvrquK2pbuhzwIoVK8o8PePs3bu3zNP50lprc+fOLfPU0p3OyfQMnz5zuo+l6+TQoUNl3lprl156aZmn8zVd62nf6R6Q2p4n6r43GfjPLwAAAN0z+QUAAKB7Jr8AAAB0z+QXAACA7pn8AgAA0L0p2/acGthSw+TRo0fLfObMmWWemhCPHTtW5qm1c1Q72tD2zLSPlCfpO0rHc+TIkTJP7Y+ttfa5z32uzFNza5KaR1OT36j2zKluaKNiah1sLX+vr7XUSvwbv/EbZX733XfHbV155ZVlPm3atDL/m7/5mzK/9dZby/yOO+4YtP1Vq1aVeTrOX/mVXynz1lp79tlnyzyNS2vWrCnzf/mXf4n7mCpSo+euXbvK/MUXXyzzpUuXDnr9RErXbmo3HSpt58CBA/E9Z555Zpmn73XoPS5dJ6PaTaeKdO9L+aZNm8o8NQO31trhw4fL/JlnninztDLCOeecE/dRSfeGdKzp2aq1/CyTztfU7p+eQdK5l+596XpIz0Sj7qHp/B7aWt+Doc8mF1xwQZmn3ye1dG/fvj3u453vfGeZp+bj9BnS75auz6Et0MePHy/z1vK1u3HjxjJP10861tT23MNzw/fjP78AAAB0z+QXAACA7pn8AgAA0D2TXwAAALpn8gsAAED3TH4BAADo3pRd6uiMM84o81Stn/K0jEZa/iLlaRmItN/Whi83k5Z3SnX/afvpWFOle6qGH7XUUTqm3bt3l3mqsk/f39jYWJn3vNTRUCdiOaN0LqW6/y9+8Ytl/ru/+7tlnpZEaK21P/7jPy7za665pswXL15c5rfcckuZP/zww2W+f//+Mj/77LPLPJ3bacmSUfbu3VvmV111VZl/8pOfHLyP10saf9KyKGkMeOKJJ8r84MGDZZ7uASfC0CWQ0jWdPsOoZTTSvtOSNumY0lifzvseljpauXJlmafvLi1BMmopqqG/T7p+0jmT7q3pfp9+z/Qs1lo+Z9JnG7o8Vtp3uh7S/SotQ5OOp7X8PJbGmfS7LV++vMyfeuqpuO+pLi3hk7679F2PGt/SOZCW5krLaY1ajmzI69O1nvbbWl6KNV0PaQzYt29fma9YsSLuu3f+8wsAAED3TH4BAADonskvAAAA3TP5BQAAoHsmvwAAAHRvyrY9p/a3JLX8pTa15557rsxT2/PQpsVRUltc+gypbTMZ2kydGhVHtdSlZt30/Q39PRcsWFDmjz/++KDtnAhDGy8nypVXXhn/tnbt2jJPzZM33nhjma9Zs6bMU2vjP/3TP5X5X//1X5f57/3e75V5a61997vfLfPU3Ll9+/YyT9doug5Ti+SuXbvKPF1X8+fPL/PWWtu8eXOZz549u8wvvPDCuK2pYu7cuWWeWjIXLVpU5tu2bSvzu+66q8zTWNJavkbTGJqu9aHSdlK7aWowTa2qreXm8HQvO/PMM8s83UNTO3APli5dOuj16fdMY1Vr+bxP596xY8fK/PDhw2Wefud0L06vH3UfG7oaQGqaTseUzvuh12Eao0c1/Q7dR/psqXG357bn9HyYWoyHNtC31tqdd95Z5qnBe+j1kJ6707Gm1VFGtd//27/9W5mnY03jSXoeG/UM0jv/+QUAAKB7Jr8AAAB0z+QXAACA7pn8AgAA0D2TXwAAALo3ZdueU/Nkam5NzWzz5s0r8+eff77Md+7cOeh4RjUipybElA+VWufSd5Ea5F544YUyT02LrbW2cePGMk8NrUlqJE3f92T0Wrc6X3HFFWX+sY99LL4nHVNqHvz5n//5Mv/0pz9d5qlRcfr06WX+l3/5l2X+3ve+t8xba+3nfu7nynzLli2Djim10m7durXMU9tzakVP3/Wzzz5b5q21tn///jJftmxZmZ922pQdyv+f1GSdxoChLampCf6CCy6IxzT02p2oZvf0+rT9NEaPGm9TG+rb3va2Mr/00kvLPN0Hhjb4TyVDV0xIv+eoNuE9e/aUeXpmSff1NO6l3y01iqdnq1HndtpHOqZkotqb0/0nfXejPlv6W9r3ySj9zqkhPq0Gku6to66f1Hx83XXXlXl6Dti0aVOZj42NlXlqVk73mdTS3lpeuSCdY0Ovq7TCQnqe6Onc9p9fAAAAumfyCwAAQPdMfgEAAOieyS8AAADdM/kFAACge1O2InTOnDllnpoKU5tfap1LrbfPPPPMD3B0/21UW2Bqqpuo5tZXXnmlzFMLZ2pzTO22o1qp0+dO70lNe6ld+6yzzor77tXv/M7vlPlVV11V5ukcbi2fA6ltMZ2T7373u8t8w4YNZZ4axZ988skyTy3TrbW2fv36Mk/n2KpVq8o8tTqnJsQFCxaU+W233Vbmqf0xHU9rudU5XdPbt28v89RuOhmlz5yajFM7dLo37N69u8zT2NNa/u3S2D1Rze5pO0NbOEcdz+HDh8t81qxZZZ4a9r/+9a+Xebp+Nm/eHI9pqhh6D01jUhpLWsst5wcOHCjz9LulZ5x0zqT22XRvGHWOpfF+6EoUqe05tdumpt+hbeyj2oTTe4aOAVNpjB4qja1pLE7S9TOqzf4v/uIvyvyrX/1qmafxMDVNp2s9nTNDt99aa1dffXWZ//Iv/3KZ79ixo8zTc0N6fbqH7tq1q8ynIv/5BQAAoHsmvwAAAHTP5BcAAIDumfwCAADQPZNfAAAAujdl255T22JqKkxtcak9MzWwpRbB1JyY8taGNx6mPH3mJG0nfbah228tt26nxtChraqp5bEH6VxNzcd33313mb/pTW+K+1i+fHmZp+bW9PqLL764zFObY2odXLduXZlfdNFFZd5abrNO7eSpOfzIkSNlPn/+/DK/5557yvyGG24o87Vr15b5pZdeWuat5fM+jSfp2l24cGHcx2ST2ptTy2z6bOnc27NnT5mnRtrW8jmWrtGhbbJDpf2m++GoxvejR4+WeWppfuqpp8o8tZjOmzcv7nuqS2NM+p1Tc/Oo8yKdS+laT9fJ0HtrauhN99x07o2Szr30mdOxjmpjHiJ95lErWqTnqHSPS9Lv2YPUZp6eu9N5kRqxv/e978V9f/7zn/8+R/eDSdfuRNm3b1/82xe+8IUyf8c73lHm6TxODfHp+knP6dqeAQAAYAox+QUAAKB7Jr8AAAB0z+QXAACA7pn8AgAA0L3u2p6T1KqYmoz3799f5qmlbmjD33gMbWke2hqdWm/Td51e31prL730Uplv3bq1zBctWhS3VRnVwjjZpM+WfrfUqLdt27YyTw1/9913Xzymhx56qMxTO+OaNWvK/Pnnny/zJUuWlHk61tQwO2vWrDJvLZ9/qZU2SW2/acwYGxsbtP2f/MmfHPT6UftOx5pafZ988snB+369pOshNYCm3zmN3akNd5SJamlO20nttikfeg9I96vW8niSxu6hrf/pWu9BGgPS75DuoamJubXcrjy0UTz9bkNXqBh6TraWm8DTPtL3ms7JtO/UYjv0Ohz1nJH+NvRZsOeVK1asWFHmQ8+l9PpRq6kMNbTBf6h07o26fpKhz/ZJ+mxpdY/HHnts0PYns6kzgwAAAIBxMvkFAACgeya/AAAAdM/kFwAAgO6Z/AIAANC9Kdv2nJokU+tpaltMDaC7d+8u89S2Op72taHvSXlqqUtNn6khL31Hab+j2jzPOOOMMn/kkUfKfOnSpWV+/PjxQcc0ffr0Mk+tkxNp5cqVZZ4aPVMLZ2q3Tb/bWWedVeZnnnlmmbeW25X37NlT5vv27Svz1Eydzr30u6XW6G9+85tl3lprH//4x8v8vPPOK/P169eXefpe9+7dW+YLFy4s802bNpX5d7/73TJP52prueV6xowZZb5u3bq4rakiNbSmcym1faffLTXxjmrzHNr2PFHt0BO1/VFNsuned+DAgTJP41X6/tLv2YN07abxLV3PafxsLY/F6XtNeTrv0/07XW+pcXlU4+7Q+0DK03PAzJkzyzydq2nMmD9/fpmP53oeNa5Xem57Ts916Xsdmr/wwgvjO7DCeFqXXy9pbEjPgqmBPI3dy5YtG9+BTSH+8wsAAED3TH4BAADonskvAAAA3TP5BQAAoHsmvwAAAHRvyrY9pxbB1OqbGvhSi2Bq6E0NuqmhdzxS61xqW0wN1KkFeuh+U1vkqKa9OXPmlHlqt0z7Tg2W6bOl9scT0fac2lNTQ+9zzz1X5ulcvffee8v8fe97X5mPai9M32tq+du2bVuZp6bKdD2k7+Lv//7vy/zmm28u89Zae//731/mP/uzP1vmqekzNV+n9vN03qeW6bGxsTJPDdettbZ///4yv+GGG8o8nRtLliyJ+5hs0viWzrHUVju0qX9UK3FqwxzVEP16SMczqpE/jaGPPvpomS9atKjMUxPrqH33Kv0O6XkljT2t5ZUr0u82tCk3NcAObeke1Yg8tO057TudS+k5La1OkJr60xidWnVbyw3e6bOl7yntuwfpeTl9F0PH1TTW9y49gyxevHjQdtK5Om/evMHHNNX4zy8AAADdM/kFAACgeya/AAAAdM/kFwAAgO6Z/AIAANA9k18AAAC6N6mXOkpLXLTW2ssvv1zmaRmAtNRNWpJn9uzZg7aT9nvs2LEyby3X96dtTdTSRUNfn5Z8SUv7tJaXFUnfd/qe0nbS7/96Lq+Rlq5ZvXr1oO088cQTZZ6WdZg/f36Zp+VyWstLDaQK/bRkRdpOun7SslnXX399mf/RH/1RmY/Hgw8+WOb33HNPmf/jP/5jmf/rv/5rmaelQ9I5POr3ScsypWvxyiuvLPP169fHfUw26ZpO0lIjjz32WJmnZbaG7re14cvKTNTyNEkau0ctHZKuxaeffrrMV6xYUebpWEfdv6e6dO2mpXrS7zPqnp6WQUq/6YEDB8p87969ZZ7G9LT9dJ2ksaq1fI2mcyZ9hrQkZVoeLT1npGUE05JGo36fNBanz5C+1/QZepCW2kufOX1H6bpK3/Uo6bwfz31gIqTjaS0fUxp/hl7T6XdYunRpPKZe+M8vAAAA3TP5BQAAoHsmvwAAAHTP5BcAAIDumfwCAADQvUnd9jyqaW9oe2Jq5kstcmk76fVD2zkno6GN1aOalVMTdGoYTdtKbXcpP+OMM+IxvV4eeuihMr/kkkvKPDVbfuUrXynz1Gx5/vnnx2M666yzyvy8884r8/T7bNmypczT759aoJ988skyv+2228q8tdy6vHHjxjI/evRo3NYQY2NjZZ7O7TRWLVy4MO5j2bJlg44pfeapJH1PqTU4jUv/+Z//WeZnnnlmmY9q+RzaPpqksTV95pSn+8zQZv/Wcgv5Aw88UObXXHNNmaeW8x7uiUlqSU2/83gaZlOTbRqL0++Z7okT9buNaqtNhjYcp+e3NDakz5xWTEjfXRozWsvfXxoz0vea7vc9WLx4cZnv3LmzzIc+d49aMWGqGLqKS2ut7d69e9Dr0zk5nmeTXvjPLwAAAN0z+QUAAKB7Jr8AAAB0z+QXAACA7pn8AgAA0L1J3facGvhaG95uOZ5GtSGGtk9P5D6Gmqjm0fF8ttSUm/YxtBVyMrY9p0bPDRs2lPmSJUvKPH0XH/nIR8Z3YAOsWrWqzFPj5RNPPFHmJ6IBdu7cuWWeGq7TOJNaTIdeJykf1fJ5//33l/mhQ4fie6a69D2lcyy1az/zzDNlvnz58kHbb214E+vQFuhRbb8Tsd9Rny2NlXfccUeZp7b0dP300MSa7N27d9Dr09idWlhbyw3H6T2p+Xj69OllnsbidM6MOpeS9J6ZM2eWeTqP03eRpLFkxowZZZ6eS0Zd/+naHfq77dixI+5jqht67o26HiqpvXsqGc9z/dDPna6rdH6n66Qn/vMLAABA90x+AQAA6J7JLwAAAN0z+QUAAKB7Jr8AAAB0b1K3PadGwNZyQ9rQ9szUzHfgwIFBrx9P83FqYEvbmqjm6KHtcqkV9MUXX4zvSU2sqf3v2LFjZT6q8XsiXj8ZPfvss4Nen86LUddPalVM58bWrVvLPLWYpnMmNZKmJtlR5+rhw4fLfM+ePYPyHqTf80S0a0+UN77xjWWezpk0xqR7wNKlS8t83rx58ZjS95qawNO1mLYzUc376XeeM2dOmbfW2pEjR8p86D00/Q5Dm/qnktTGnn7PdL6M+q7T80F6T8pT43K6roY+44z6ndPnTtL5nZ4P0nk/9DpJRrW3p+8vSWNAD9dJ+mzpeSx95qFN2fv27fsBju5/mkr3xCSt+jD0uS6NGek5fTyrCkxW/vMLAABA90x+AQAA6J7JLwAAAN0z+QUAAKB7Jr8AAAB0b1K3PafG2NaGt22m/Pjx42U+f/78Mt+8eXOZz507t8xHtTkObbxLjWqpgS19RylPjXDpOxrVhHjuueeW+Xe+850yX7lyZZmnY03fXQ9tz0Ol323//v0n+Ej+W2ojTDn/Oz00WKZrNzUWp3MptYBv3769zFOzZWt5/E7j0tAG/6GtzkPbPFNLbmu5/Tx9f1u2bCnz1Ho7tDV6KqP92YwAAAVcSURBVEnPAem+lBqxR7Wkpmt66LmUpLbvtIpD2v6oVSjS5x5q6GdOv0NqlB963baWx420ckHy+OOPD3r9ZDSqMb8y9H6Vxr20KstUMp57d2q5TveltI+Up3Ep3Yt3795d5pOZ//wCAADQPZNfAAAAumfyCwAAQPdMfgEAAOieyS8AAADdm9Rtz6n1uLXhLZypOfpLX/pSmf/ET/xEmV9yySVl/vDDD5f5qLbD1CSYPtt4GgmHvD4156VGxR/7sR+L+/iP//iPMr/vvvvK/Pzzzy/zod/RqIZwYPIa2iqfmoxTW3Ea6/nBpHbbdM8d1WTcqwcffLDMFy1aVObpnG8tPzuk62Hoqg+pEXlo03jKR21rogxd3WPoKiGjVrRIrc6pUTq106cW9alk2bJlZZ7O4fT8NnS1jqeeemrQ61ubfCsjjOd4nn766TIfeu0OHaOXL19e5tqeAQAAYBIy+QUAAKB7Jr8AAAB0z+QXAACA7pn8AgAA0L1J3fY8qvktNeeltrPU8rdr164y//KXv1zm1113XZlffvnlZb59+/Yyby23vKXP9sILL5T5tGnTynxsbKzMUwPf7Nmzy/yCCy4o83vvvbfMW2vtrrvuin+rpN96aNOrtmeYmtIYkMal1OrMayONxTNnzizzk7Ht+cMf/nCZ33777WU+6hxOzwepKTe1Nyfp9xnV3lwZtdpEeu5Kn21o8+3QdtuhK2mkZ6vW8ng1Z86cMv/VX/3VuK2pLjXBpzbm9Ayavu/0O6dn4lF6aHtOK7Okazo9F6frYevWrWU+tI17MvOfXwAAALpn8gsAAED3TH4BAADonskvAAAA3TP5BQAAoHuTuu154cKFg9+TWuSOHz/+vz2c1lprf/u3f1vmixYtKvPLLrssbis1p82fP7/M586dW+apCS9tPzXCpdbG9evXl/kTTzxR5uORmipPP/30Mk8No6m9DpjcUutlGsdS4yWvjdSsmu5LL7744mt5OJPSt771rTK/+eaby3zt2rVxW+mZJa3ikJ4D0j0x3e9TQ3NqUB7VVjt032kfqY05bWfoqh9pv2lViVH7/sIXvlDmf/qnfxq3NdW99a1vLfOLL764zB944IEynzVrVpkfPXp0fAdW6KHteei20vNyctFFFw3K77nnnkHbnwz85xcAAIDumfwCAADQPZNfAAAAumfyCwAAQPdMfgEAAOieyS8AAADdm9RLHaUldlrLyyukJYcmsiq9sn379jL/yle+Et8zffr0Ml+wYEGZpyU/Un1/quLfuXNnme/YsaPMT4Rt27aVefrdNm7cWOYn4/Ia0IO0XEIaD6dNm/ZaHg7/n0ceeaTM033m0UcffS0PZ0r56Ec/Wub//M//HN/zUz/1U2V+8ODBMl+yZEmZp+UC03bS7zmeJVnSso3pvn7o0KEyT0sXpc+QlsxJSy+lczUtndlaXtZq06ZN8T29uuWWW8p8w4YNZX7OOeeU+erVqyfsmE5GafnRtJRoOlf/8A//sMzTElVTkf/8AgAA0D2TXwAAALpn8gsAAED3TH4BAADonskvAAAA3TtlPA1+/9e6devG/2YAAAAYaN26daeM533+8wsAAED3TH4BAADonskvAAAA3TP5BQAAoHsmvwAAAHTP5BcAAIDumfwCAADQPZNfAAAAumfyCwAAQPdMfgEAAOieyS8AAADdO+XVV199vY8BAAAAXlP+8wsAAED3TH4BAADonskvAAAA3TP5BQAAoHsmvwAAAHTP5BcAAIDumfwCAADQPZNfAAAAumfyCwAAQPdMfgEAAOieyS8AAADdM/kFAACgeya/AAAAdM/kFwAAgO79H0+tWU+NGVBhAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1200x900 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import numpy as np\n",
    "\n",
    "# Functions to show an image\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    \n",
    "    figure(num=None, figsize=(8, 6), dpi=150, edgecolor='k')\n",
    "    plt.axis('off')\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, xdim, conv1_fksp, conv2_fksp, conv3_fksp, pool_ksp, fc1_out, fc2_out, dropout, batch_norm): \n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=conv1_fksp[\"filter\"], kernel_size=conv1_fksp[\"kernel\"], \n",
    "                               stride=conv1_fksp[\"stride\"], padding=conv1_fksp[\"padding\"])\n",
    "        self.conv2 = nn.Conv2d(in_channels=conv1_fksp[\"filter\"], out_channels=conv2_fksp[\"filter\"], kernel_size=conv2_fksp[\"kernel\"],\n",
    "                               stride=conv2_fksp[\"stride\"], padding=conv2_fksp[\"padding\"])\n",
    "        self.conv3 = nn.Conv2d(in_channels=conv2_fksp[\"filter\"], out_channels=conv3_fksp[\"filter\"], kernel_size=conv3_fksp[\"kernel\"],\n",
    "                               stride=conv3_fksp[\"stride\"], padding=conv3_fksp[\"padding\"])\n",
    "        self.pool = nn.AvgPool2d(pool_ksp[\"kernel\"], pool_ksp[\"stride\"], pool_ksp[\"padding\"])\n",
    "        self.conv1_bn = nn.BatchNorm2d(conv1_fksp[\"filter\"])\n",
    "        self.conv2_bn = nn.BatchNorm2d(conv2_fksp[\"filter\"])\n",
    "        self.conv3_bn = nn.BatchNorm2d(conv3_fksp[\"filter\"])\n",
    "        self.Dropout = nn.Dropout(dropout)\n",
    "\n",
    "        dconv1=int((xdim+2*conv1_fksp[\"padding\"]-(conv1_fksp[\"kernel\"]-conv1_fksp[\"stride\"]))/conv1_fksp[\"stride\"])\n",
    "        dpool1=int((dconv1+2*pool_ksp[\"padding\"]-(pool_ksp[\"kernel\"]-pool_ksp[\"stride\"]))/pool_ksp[\"stride\"])\n",
    "        dconv2=int((dpool1+2*conv2_fksp[\"padding\"]-(conv2_fksp[\"kernel\"]-conv2_fksp[\"stride\"]))/conv2_fksp[\"stride\"])\n",
    "        dpool2=int((dconv2+2*pool_ksp[\"padding\"]-(pool_ksp[\"kernel\"]-pool_ksp[\"stride\"]))/pool_ksp[\"stride\"])\n",
    "        dconv3=int((dpool2+2*conv3_fksp[\"padding\"]-(conv3_fksp[\"kernel\"]-conv3_fksp[\"stride\"]))/conv3_fksp[\"stride\"])\n",
    "        dpool3=int((dconv3+2*pool_ksp[\"padding\"]-(pool_ksp[\"kernel\"]-pool_ksp[\"stride\"]))/pool_ksp[\"stride\"])\n",
    "        self.FCinput = conv3_fksp[\"filter\"] * dpool3 * dpool3\n",
    "\n",
    "        self.batch_norm = batch_norm\n",
    "        self.fc1 = nn.Linear(self.FCinput, fc1_out)\n",
    "        self.fc2 = nn.Linear(fc1_out, fc2_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.conv1_bn(x) if self.batch_norm == True else x\n",
    "        x = self.pool(F.relu(self.conv2(x))) \n",
    "        x = self.conv2_bn(x) if self.batch_norm == True else x\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.conv3_bn(x) if self.batch_norm == True else x\n",
    "        x = self.Dropout(x)\n",
    "\n",
    "        x = x.view(-1, self.FCinput)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #x = F.softmax(self.fc2(x),1) #dont need softmax here again\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inputs\n",
    "\n",
    "# Input dimension\n",
    "xdim = trainset[0][0].shape[1]\n",
    "\n",
    "# Parameters (fixed)\n",
    "conv1_fksp_base_ls = [32,5,1,2]\n",
    "conv2_fksp_base_ls = [32,5,1,2]\n",
    "conv3_fksp_base_ls = [64,5,1,2]\n",
    "fc1_out_base = 64\n",
    "fc2_out_base =10\n",
    "\n",
    "def createConvDic(ls):\n",
    "    dic = {}\n",
    "    dic[\"filter\"] = int(ls[0])\n",
    "    dic[\"kernel\"] = int(ls[1])\n",
    "    dic[\"stride\"] = int(ls[2])\n",
    "    dic[\"padding\"] = int(ls[3])\n",
    "    return dic\n",
    "\n",
    "conv1_fksp_base = createConvDic(conv1_fksp_base_ls)\n",
    "conv2_fksp_base = createConvDic(conv2_fksp_base_ls)\n",
    "conv3_fksp_base = createConvDic(conv3_fksp_base_ls)\n",
    "\n",
    "\n",
    "# Parameters (tuning)\n",
    "pool_ksp_base1_ls = [2,2,0]\n",
    "pool_ksp_base2_ls = [4,3,1]\n",
    "dropout = [0.1,0.3,0.5]\n",
    "batch_norm = [True,False]\n",
    "\n",
    "def createPoolDic(ls):\n",
    "    dic = {}\n",
    "    dic[\"kernel\"] = int(ls[0])\n",
    "    dic[\"stride\"] = int(ls[1])\n",
    "    dic[\"padding\"] = int(ls[2])\n",
    "    return dic\n",
    "\n",
    "pool_ksp_base1 = createPoolDic(pool_ksp_base1_ls)\n",
    "pool_ksp_base2 = createPoolDic(pool_ksp_base2_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save all 12 nets\n",
    "nets_init =[]\n",
    "tuning_params = []\n",
    "for i, x in enumerate([pool_ksp_base1, pool_ksp_base2]):\n",
    "    for j, y in enumerate(dropout):\n",
    "        for k, z in enumerate(batch_norm):\n",
    "            tuning_params.append([x,y,z])\n",
    "            exec(\"nets_init.append(Net(xdim = xdim, conv1_fksp = conv1_fksp_base, conv2_fksp = conv2_fksp_base, conv3_fksp = conv3_fksp_base, pool_ksp = x, fc1_out = fc1_out_base, fc2_out = fc2_out_base, dropout=y, batch_norm=z))\")\n",
    "\n",
    "len(nets_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preserve the initialization\n",
    "nets = nets_init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def train_CNN_MNIST(model, model_name, epochNum):\n",
    "    \n",
    "    # Train model\n",
    "    model.train()\n",
    "    \n",
    "    # Create logger and prepare for logging\n",
    "    logdir = os.path.join(\"logs\", model_name, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    logger = Logger(logdir)\n",
    "    overall_step = 0\n",
    "    \n",
    "    # initialize criterion and optimizer \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9) #momentum=0.9\n",
    "\n",
    "    # Set number of epochs\n",
    "    epochs = epochNum\n",
    "\n",
    "    # Start looping over epochs\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        running_loss = 0\n",
    "        epoch_loss = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        loss_200 = []\n",
    "        accuracy_200 = []\n",
    "        \n",
    "        # Start looping for all batches\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            \n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels) #forward\n",
    "            loss.backward() #backward\n",
    "            optimizer.step() #optimize\n",
    "\n",
    "            # pred = torch.nn.functional.softmax(pred, dim=1)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += predicted.eq(labels.reshape(len(labels),)).sum() \n",
    "            total += float(len(labels))\n",
    "            accuracy = correct / total\n",
    "\n",
    "            # print statistics and save loss and accuracy for every 200 batches\n",
    "            running_loss += loss.item()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "            if i % 200 == 199:    # print every 200 mini-batches\n",
    "                \n",
    "                # print('Epoch: %d, Batch: %5d, loss: %.3f' %\n",
    "                #       (epoch + 1, i + 1, running_loss / 200))\n",
    "                \n",
    "                running_loss = 0.0\n",
    "\n",
    "                loss_200.append(loss.item())\n",
    "                accuracy_200.append(accuracy.item())\n",
    "\n",
    "                info = {'loss' : loss.item(), 'accuracy': accuracy.item()}\n",
    "                for tag, value in info.items():\n",
    "                    overall_step+=1\n",
    "                    logger.scalar_summary(tag, value, overall_step)\n",
    "        \n",
    "        # Print avg loss for every epoch\n",
    "        print('Epoch ',epoch,'; Epoch Avg Loss: ', epoch_loss/len(trainloader), '; Loss Item: ', loss.item()) #\n",
    "    \n",
    "    \n",
    "    # Save model, loss and accuracy for every 200 batches\n",
    "    PATH='./model_results/'+model_name+'.pt'\n",
    "    torch.save({\"state\":model.state_dict(),\n",
    "                \"loss_200\":loss_200,\n",
    "                \"accuracy_200\":accuracy_200}, PATH)\n",
    "    \n",
    "    # Finish Msg\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform Training and Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 ; Epoch Avg Loss:  0.4257061731192128 ; Loss Item:  0.008327875286340714\n",
      "Epoch  1 ; Epoch Avg Loss:  0.2980645851070318 ; Loss Item:  0.030627692118287086\n",
      "Epoch  2 ; Epoch Avg Loss:  0.2597943559924582 ; Loss Item:  0.15832750499248505\n",
      "Epoch  3 ; Epoch Avg Loss:  0.2348552935316875 ; Loss Item:  0.005805750377476215\n",
      "Epoch  4 ; Epoch Avg Loss:  0.21492399987222233 ; Loss Item:  0.17290128767490387\n",
      "Epoch  5 ; Epoch Avg Loss:  0.19828825651185647 ; Loss Item:  0.15373003482818604\n",
      "Epoch  6 ; Epoch Avg Loss:  0.18268045453888831 ; Loss Item:  0.26484328508377075\n",
      "Epoch  7 ; Epoch Avg Loss:  0.17098416325347407 ; Loss Item:  0.05962168052792549\n",
      "Epoch  8 ; Epoch Avg Loss:  0.16146739040640615 ; Loss Item:  0.7346031665802002\n",
      "Epoch  9 ; Epoch Avg Loss:  0.14960821227911902 ; Loss Item:  5.430599412648007e-05\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "train_CNN_MNIST(nets[0], \"model0\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-6521f203bc4aa162\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-6521f203bc4aa162\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_CNN_MNIST(model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    #TODO : Report this accuracy in your report.\n",
    "\n",
    "    print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "        100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 91 %\n"
     ]
    }
   ],
   "source": [
    "test_CNN_MNIST(nets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_per_class(model):   \n",
    "    model.eval()\n",
    "    class_correct = list(0. for i in range(10))\n",
    "    class_total = list(0. for i in range(10))\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            c = (predicted == labels).squeeze()\n",
    "            for i in range(4):\n",
    "                label = labels[i]\n",
    "                class_correct[label] += c[i].item()\n",
    "                class_total[label] += 1\n",
    "\n",
    "\n",
    "    for i in range(10):\n",
    "        print('Accuracy of %5s : %2d %%' % (\n",
    "            classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of T-shirt/top : 83 %\n",
      "Accuracy of Trouser : 97 %\n",
      "Accuracy of Pullover : 87 %\n",
      "Accuracy of Dress : 89 %\n",
      "Accuracy of  Coat : 81 %\n",
      "Accuracy of Sandal : 98 %\n",
      "Accuracy of Shirt : 78 %\n",
      "Accuracy of Sneaker : 94 %\n",
      "Accuracy of   Bag : 98 %\n",
      "Accuracy of Ankle boot : 97 %\n"
     ]
    }
   ],
   "source": [
    "acc_per_class(nets[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
